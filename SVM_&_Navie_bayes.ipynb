{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM & Navie bayes**"
      ],
      "metadata": {
        "id": "QD-rOqC0x2K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions 1:** What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "**Answers:-** Information Gain is a metric used in decision trees to decide which feature to split on at each node. It measures how much uncertainty (entropy) is reduced when the dataset is split based on a particular attribute. The attribute with the highest Information Gain is chosen for the split, ensuring the tree becomes more efficient and accurate.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mrfHGtO_x8Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions 2:** What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "**Answes:-** Both Gini Impurity and Entropy are measures of node impurity used in decision trees. The key difference is that Entropy comes from information theory and measures disorder using logarithms, while Gini Impurity is simpler, measuring the probability of misclassification. Gini is computationally faster, while Entropy can lead to more balanced splits.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X-LUttexz3rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "**Answers:- **Pre-pruning (also called early stopping) is a technique in decision trees where the growth of the tree is stopped early—before it becomes fully grown—to prevent overfitting. It sets constraints (like maximum depth, minimum samples per node, or minimum information gain) so the tree doesn’t become too complex and generalizes better to unseen data.\n"
      ],
      "metadata": {
        "id": "PuhUaD4S5ENQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions 4:** Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "RWDexkOV6VDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answers:-\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Display results in a neat table\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Feature Importances (using Gini Impurity):\")\n",
        "print(importance_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkcvVMPfx10k",
        "outputId": "338f5862-cb96-4eff-dd4c-92cfaa4d104a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (using Gini Impurity):\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions 5:-** What is a Support Vector Machine (SVM)?\n",
        "\n",
        "**Answers:-** A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression. It works by finding the optimal boundary (called a hyperplane) that best separates different classes in the data, while maximizing the margin between them.\n"
      ],
      "metadata": {
        "id": "zNybm1a2-_Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions 6:-** What is the Kernel Trick in SVM?\n",
        "\n",
        "**Answers :-** The Kernel Trick in SVM allows us to solve non-linear classification problems by implicitly mapping data into a higher-dimensional space without explicitly computing the transformation. It enables Support Vector Machines (SVMs) to find linear decision boundaries in complex datasets by using kernel functions instead of manual feature mapping.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HbG8LY5DoODU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions 7:- Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "Answers :-"
      ],
      "metadata": {
        "id": "GRJR7G7oo6Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answers:-\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "x, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "#print\n",
        "\n",
        "print(\"Accuracy with Linear Kernel: {:.2f}\".format(acc_linear))\n",
        "print(\"Accuracy with RBF Kernel: {:.2f}\".format(acc_rbf))\n",
        "\n",
        "#compare\n",
        "\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"Linear kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"Both kernels performed equally well.\")"
      ],
      "metadata": {
        "id": "Vlsq_6od-0Ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2172c9-f3a6-48c4-aff8-59d9d66827c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.96\n",
            "Accuracy with RBF Kernel: 0.98\n",
            "RBF kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6Zl1dGZrGv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answers:- The Naïve Bayes classifier is a simple yet powerful probabilistic machine learning algorithm based on Bayes’ Theorem. It is widely used for classification tasks such as spam detection, sentiment analysis, and text categorization.\n"
      ],
      "metadata": {
        "id": "b4UJD5eZrdNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "Answers:-\n",
        "1. Gaussian Naïve Bayes\n",
        "- Assumption: Features are continuous and follow a normal (Gaussian) distribution within each class.\n",
        "- Use Case: Works well with datasets where features are real-valued (e.g., height, weight, sensor readings).\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "- Assumption: Features are discrete counts (non-negative integers).\n",
        "- Use Case: Commonly used in text classification (spam detection, sentiment analysis) where features represent word counts or term frequencies.\n",
        "- where N_{iy} is the count of feature i in class y.\n",
        "- Example: Predicting whether an email is spam based on word frequency.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "- Assumption: Features are binary (0 or 1), indicating presence/absence of a feature.\n",
        "- Use Case: Useful when only the existence of a feature matters, not its frequency.\n",
        "\n",
        "- where p_y is the probability of feature presence in class y.\n",
        "- Example: Classifying documents based on whether certain keywords appear at all.\n",
        "\n"
      ],
      "metadata": {
        "id": "jMpXLSUrryeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Breast Cancer Dataset**\n",
        "\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "1wEOwHAjs6eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answers:-\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features (optional but often helpful)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: {:.2f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRZBU-VJrxqN",
        "outputId": "8576820c-475b-4d34-b87b-ffcf76c09497"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes on Breast Cancer dataset: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WUi1PzHtRxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}